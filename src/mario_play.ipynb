{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Super Mario Gym\n",
    "This jupyter notebook is used to help you understand the Super Mario Gym environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install -r requirements.txt # Or use your virtual environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obs shape: (4, 13, 16), reward: 0.0, done: False, episode_info: None\n"
     ]
    }
   ],
   "source": [
    "from game import *\n",
    "\n",
    "env = Mario(n_stack=4)\n",
    "state = env.reset()\n",
    "obs, reward, done, episode_info = env.step(env.env.action_space.sample())\n",
    "n_stack = obs.shape[0]\n",
    "print(f\"Obs shape: {obs.shape}, reward: {reward}, done: {done}, episode_info: {episode_info}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABFEAAADTCAYAAABJLoxnAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuNSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/xnp5ZAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAGvklEQVR4nO3avW1TYRSAYQc5G5CGmgFAaWACWhfMgEjLBJ6ANogpUrECFYI9QkOdwvSRQG+u4/uT+zy1P/lY1z6yXn1nh8PhsAEAAADgv55NPQAAAADAEogoAAAAAIGIAgAAABCIKAAAAACBiAIAAAAQiCgAAAAAgYgCAAAAEIgoAAAAAMG2vvD1x8+nnAN4RD+/fJp6hMHsGlgOuwYYg10DjKHuGjdRAAAAAAIRBQAAACAQUQAAAAACEQUAAAAgEFEAAAAAAhEFAAAAIBBRAAAAAAIRBQAAACAQUQAAAAACEQUAAAAgEFEAAAAAAhEFAAAAIBBRAAAAAAIRBQAAACAQUQAAAAACEQUAAAAgEFEAAAAAAhEFAAAAIBBRAAAAAAIRBQAAACAQUQAAAAACEQUAAAAgEFEAAAAAAhEFAAAAIBBRAAAAAAIRBQAAACAQUQAAAAACEQUAAAAgEFEAAAAAAhEFAAAAIBBRAAAAAAIRBQAAACAQUQAAAAACEQUAAAAgEFEAAAAAAhEFAAAAIBBRAAAAAAIRBQAAACAQUQAAAAACEQUAAAAgEFEAAAAAAhEFAAAAIBBRAAAAAILt1APAQ5zvbgefvbu5eMRJgLmzL4Ax2DXAGOya+XATBQAAACAQUQAAAAACEQUAAAAgEFEAAAAAAhEFAAAAIBBRAAAAAAIRBQAAACAQUQAAAAACEQUAAAAgEFEAAAAAAhEFAAAAIBBRAAAAAAIRBQAAACAQUQAAAACC7dQDwEPc3VxMPQKwEPYFMAa7BhiDXTMfbqIAAAAABCIKAAAAQCCiAAAAAAQiCgAAAEAgogAAAAAEIgoAAABAIKIAAAAABCIKAAAAQCCiAAAAAAQiCgAAAEAgogAAAAAEIgoAAABAIKIAAAAABCIKAAAAQCCiAAAAAAQiCgAAAEAgogAAAAAEIgoAAABAIKIAAAAABCIKAAAAQCCiAAAAAAQiCgAAAEAgogAAAAAEIgoAAABAIKIAAAAABCIKAAAAQCCiAAAAAAQiCgAAAEAgogAAAAAEIgoAAABAIKIAAAAABCIKAAAAQCCiAAAAAAQiCgAAAEAgogAAAAAEIgoAAABAIKIAAAAABCIKAAAAQCCiAAAAAAQiCgAAAEAgogAAAAAEIgoAAABAIKIAAAAABNupB3hqzne3g87d3Vw88iSnN/SzrskSnyvjOeY3tLTvln3RLO25sgx2Dfct7bmyDHYN9y3tuVZuogAAAAAEIgoAAABAIKIAAAAABCIKAAAAQCCiAAAAAAQiCgAAAEAgogAAAAAEIgoAAABAIKIAAAAABCIKAAAAQCCiAAAAAAQiCgAAAEAgogAAAAAEIgoAAABAsJ16gKfm7uZi6hFGs6bPCqewpt/Qmj4rzM2afn9r+qwwN0v7/f3YXw8+e7m/esRJWBo3UQAAAAACEQUAAAAgEFEAAAAAAhEFAAAAIBBRAAAAAAIRBQAAACAQUQAAAAACEQUAAAAgEFEAAAAAAhEFAAAAIBBRAAAAAAIRBQAAACAQUQAAAAACEQUAAAAg2E49AAAAAIzpcn819QgslJsoAAAAAIGIAgAAABCIKAAAAACBiAIAAAAQiCgAAAAAgYgCAAAAEIgoAAAAAIGIAgAAABCIKAAAAACBiAIAAAAQiCgAAAAAgYgCAAAAEIgoAAAAAIGIAgAAABBspx6A4/3YXw8+e7m/Gnz2+dfvg879/vB28HsC07FrgDEcs2vevXg1+KydAesy9P/FZmNfrJ2bKAAAAACBiAIAAAAQiCgAAAAAgYgCAAAAEIgoAAAAAIGIAgAAABCIKAAAAACBiAIAAAAQiCgAAAAAgYgCAAAAEIgoAAAAAIGIAgAAABCIKAAAAADBtr7wfHd7yjk4wptf7wefPea5/tm9HPaeG98l/s2umS+7hqfErpmvY3bN5tvwo3YGp2DXzNfQ/xebjX2xdm6iAAAAAAQiCgAAAEAgogAAAAAEIgoAAABAIKIAAAAABCIKAAAAQCCiAAAAAAQiCgAAAEAgogAAAAAEIgoAAABAIKIAAAAABCIKAAAAQCCiAAAAAAQiCgAAAEAgogAAAAAEIgoAAABAIKIAAAAABCIKAAAAQCCiAAAAAAQiCgAAAEAgogAAAAAEIgoAAABAIKIAAAAABCIKAAAAQCCiAAAAAAQiCgAAAEAgogAAAAAEIgoAAABAIKIAAAAABGeHw+Ew9RAAAAAAc+cmCgAAAEAgogAAAAAEIgoAAABAIKIAAAAABCIKAAAAQCCiAAAAAAQiCgAAAEAgogAAAAAEIgoAAABA8BdklHhC7F/HigAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 1400x1000 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualize the environment\n",
    "# Get the initial observation\n",
    "# test env_wrap\n",
    "import matplotlib.pyplot as plt\n",
    "done = True\n",
    "for i in range(150):\n",
    "    if done:\n",
    "        state = env.reset()\n",
    "    state, reward, done, info = env.step(env.action_space.sample())\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(1, n_stack, figsize=(14,10))\n",
    "for i in range(n_stack):\n",
    "    ax[i].imshow(state[n_stack-i-1,:,:], vmin=-1, vmax=2)\n",
    "    ax[i].axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/junweilu/Dropbox/Teach/BST236-Computing/homework/HW_9/src/model.py:194: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(path)\n"
     ]
    }
   ],
   "source": [
    "# Select a pre-trained model\n",
    "from game import Mario\n",
    "from model import load_model\n",
    "import os\n",
    "\n",
    "MODEL_DIR = '../results/ppo_experiment_gif/checkpoints/'\n",
    "MODEL_NAME = 'model_checkpoint_epoch_1.pt'\n",
    "env = Mario()\n",
    "model = load_model(os.path.join(MODEL_DIR, MODEL_NAME), env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgym_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m play_episode\n\u001b[0;32m----> 2\u001b[0m \u001b[43mplay_episode\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Dropbox/Teach/BST236-Computing/homework/HW_9/src/utils/gym_utils.py:200\u001b[0m, in \u001b[0;36mplay_episode\u001b[0;34m(env, model, episodes, deterministic, render, return_eval)\u001b[0m\n\u001b[1;32m    197\u001b[0m             action \u001b[38;5;241m=\u001b[39m pi\u001b[38;5;241m.\u001b[39msample()\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    199\u001b[0m \u001b[38;5;66;03m# Step environment\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m states, reward, done, info \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    201\u001b[0m total_reward \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m reward\n\u001b[1;32m    202\u001b[0m steps \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m~/Dropbox/Teach/BST236-Computing/homework/HW_9/src/game.py:86\u001b[0m, in \u001b[0;36mMario.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m, action):\n\u001b[1;32m     76\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;124;03m    ### Step\u001b[39;00m\n\u001b[1;32m     78\u001b[0m \u001b[38;5;124;03m    Executes `action` for n_skip time steps and\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[38;5;124;03m    * `episode_info`: episode information if completed\u001b[39;00m\n\u001b[1;32m     85\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 86\u001b[0m     obs, reward, done, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     87\u001b[0m     obs \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mtranspose(obs, (\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m))  \u001b[38;5;66;03m# Reshape to (n_stack, height, width)\u001b[39;00m\n\u001b[1;32m     89\u001b[0m     \u001b[38;5;66;03m# maintain rewards for each step\u001b[39;00m\n",
      "File \u001b[0;32m~/Dropbox/Teach/BST236-Computing/homework/HW_9/venv/lib/python3.8/site-packages/gym/core.py:323\u001b[0m, in \u001b[0;36mObservationWrapper.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    322\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m, action):\n\u001b[0;32m--> 323\u001b[0m     observation, reward, done, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    324\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobservation(observation), reward, done, info\n",
      "File \u001b[0;32m~/Dropbox/Teach/BST236-Computing/homework/HW_9/venv/lib/python3.8/site-packages/nes_py/wrappers/joypad_space.py:74\u001b[0m, in \u001b[0;36mJoypadSpace.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;124;03mTake a step using the given action.\u001b[39;00m\n\u001b[1;32m     61\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     71\u001b[0m \n\u001b[1;32m     72\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;66;03m# take the step and record the output\u001b[39;00m\n\u001b[0;32m---> 74\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_action_map\u001b[49m\u001b[43m[\u001b[49m\u001b[43maction\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Dropbox/Teach/BST236-Computing/homework/HW_9/venv/lib/python3.8/site-packages/gym/wrappers/time_limit.py:18\u001b[0m, in \u001b[0;36mTimeLimit.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m, action):\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m (\n\u001b[1;32m     16\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_elapsed_steps \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     17\u001b[0m     ), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot call env.step() before calling reset()\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 18\u001b[0m     observation, reward, done, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_elapsed_steps \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_elapsed_steps \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_max_episode_steps:\n",
      "File \u001b[0;32m~/Dropbox/Teach/BST236-Computing/homework/HW_9/venv/lib/python3.8/site-packages/nes_py/nes_env.py:300\u001b[0m, in \u001b[0;36mNESEnv.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    298\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrollers[\u001b[38;5;241m0\u001b[39m][:] \u001b[38;5;241m=\u001b[39m action\n\u001b[1;32m    299\u001b[0m \u001b[38;5;66;03m# pass the action to the emulator as an unsigned byte\u001b[39;00m\n\u001b[0;32m--> 300\u001b[0m \u001b[43m_LIB\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mStep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_env\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    301\u001b[0m \u001b[38;5;66;03m# get the reward for this step\u001b[39;00m\n\u001b[1;32m    302\u001b[0m reward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mfloat\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_reward())\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Play the game using the trained model\n",
    "from utils.gym_utils import play_episode\n",
    "play_episode(env, model) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pipenv-w_7VJq73",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
